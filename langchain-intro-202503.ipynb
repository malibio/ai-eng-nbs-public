{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI Workflows with LangChain: Prompts, Chains, and LLM Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weâ€™ll start with the basics behind prompt templates and LLMs. Weâ€™ll also explore two LLM options available from the library, using models from Hugging Face Hub or OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Note on Deprecated Warnings in LangChain  \n",
    "\n",
    "During execution, you may see **deprecation warnings** in LangChain. These warnings appear because we are using **older versions of some components** to demonstrate specific functionalities.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To0cTjhBFT0V"
   },
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cee7L_56FdMF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain, LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import FewShotPromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import inspect\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm =   OpenAI(model_name=\"gpt-3.5-turbo-instruct\", api_key=OPENAI_API_KEY, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghdws72ROs9I"
   },
   "source": [
    "### Prompts and Prompt templates âœï¸\n",
    "\n",
    "Before understanding the concept of prompt templates it is important that you must be aware about what does prompt actually means. Prompt is simply a textual instruction which we give to a model to provide some specific output.\n",
    "\n",
    "To better understand this let us assume that we want some outline about tennis. So for this our prompt could something be like \"Write me an outline on Tennis\". But what if we want to again want to get some outline about some other sport let say cricket. In such kind of scenarios the naive approach would be to simply rewrite the prompt with updated sport.\n",
    "\n",
    "But, if you are an AI Engineer you would be aware about the concept of code reproducibility and in the above mentioned naive approach this concept is getting violated as for every different city we are forced to rewrite the entire prompts with updated sport, so to make the process of creating the prompts efficient we use prompts templates.\n",
    "\n",
    "*Prompt templates are like ready-made templates which contains contextual information about the input parameter, where input parameter is simply the input provided by the end user. The below mentioned code snipped will help you understand how we can create prompts efficiently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a Prompt\n",
    "\n",
    "A prompt can consist of multiple components:\n",
    "\n",
    "* Instructions\n",
    "* External information or context\n",
    "* User input or query\n",
    "* Output indicator\n",
    "\n",
    "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
    "\n",
    "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
    "\n",
    "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
    "\n",
    "**User input or query** is typically a query directly input by the user of the system.\n",
    "\n",
    "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
    "\n",
    "Each of these components should usually be placed in the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Q2-VN6LTGBIl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : Write me an outline on Tennis?\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\"Write me an outline on {input_parameter}?\")   \n",
    "user_input = input(\"Enter sport : \")\n",
    "prompt = template.format(input_parameter=user_input)\n",
    "print(\"Prompt :\",prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : input_variables=['input_parameter'] input_types={} partial_variables={} template='Write me an outline on {input_parameter}'\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Write me an outline on {input_parameter}?\")   \n",
    "user_input = input(\"Enter sport : \")\n",
    "prompt.format(input_parameter=user_input)\n",
    "# Instantiation using initializer\n",
    "prompt = PromptTemplate(input_variables=[\"input_parameter\"], template=\"Write me an outline on {input_parameter}\")\n",
    "print(\"Prompt :\",prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wouldn't typically know what the users prompt is beforehand, so we actually want to add this in. So rather than writing the prompt directly, we create a PromptTemplate with a single input variable query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT_DGdA9VGE8"
   },
   "source": [
    "## Chains\n",
    "\n",
    "Now we are going to use a Langchain concept Chains. Chains are responsible for the entire data flow inside Langchain. As we discussed above we are passing dynamic topic input variable to OpenAI. To accommodate this we will be using a chain called LLMChain. There are a bunch of chains supported in Langchain, we will talk about them later\n",
    "\n",
    "LLMChain takes the prompt from the prompt template we created above and fills it up with the dynamic input before passing to OpenAI LLM. Let's define LLMChain below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lszkNwMNVNiV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "chain = prompt | llm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNRq_YnoQByB"
   },
   "source": [
    "Now that we have created a prompt template and a chain we can now input any topic we want. Instead of topic \"Tennis\" we can input \"Cricket\" or any other topic of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUh0JGOgPxyv",
    "outputId": "cefee482-9ba6-4313-908b-555d2c511162",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I. Introduction\n",
      "    A. Explanation of Cricket\n",
      "    B. Origins of Cricket\n",
      "    C. Popularity of Cricket worldwide\n",
      "\n",
      "II. Objectives and Rules of the Game\n",
      "    A. Objective of the game\n",
      "    B. Basic rules and terminology\n",
      "    C. Playing field and equipment\n",
      "\n",
      "III. Team Composition\n",
      "    A. Number of players\n",
      "    B. Roles and responsibilities of players\n",
      "    C. Selection process for national teams\n",
      "\n",
      "IV. Types of Matches\n",
      "    A. Test Matches\n",
      "        1. Duration and format\n",
      "        2. Importance in international cricket\n",
      "    B. One Day International (ODI) Matches\n",
      "        1. Duration and format\n",
      "        2. Evolution and popularity of ODI cricket\n",
      "    C. Twenty20 (T20) Matches\n",
      "        1. Duration and format\n",
      "        2. Introduction and growth of T20 cricket\n",
      "\n",
      "V. Scoring System\n",
      "    A. Runs and boundaries\n",
      "    B. Ways to get out\n",
      "    C. Importance of scoring rate in different formats\n",
      "\n",
      "VI. Strategy and Tactics\n",
      "    A. Batting techniques\n",
      "    B. Bowling variations and tactics\n",
      "    C. Fielding positions and tactics\n",
      "\n",
      "VII. Major International Tournaments\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input_parameter\": \"Cricket\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVBNjz2OSzrs"
   },
   "source": [
    "Now let's extend it for a multi-input prompt. Let's generate an introductory paragraph to a blog post with variables title, audience and tone of voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vqVEYaPPGJuM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"audience\", \"tone\"],\n",
    "    template=\"\"\"This program will generate an introductory paragraph to a blog post given a blog title, audience, and tone of voice\n",
    "\n",
    "    Blog Title: {title}\n",
    "    Audience: {audience}\n",
    "    Tone of Voice: {tone}\"\"\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJgvUP0pSVat",
    "outputId": "56838235-2e98-4580-9301-6b0808f89a5e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Welcome, millennial adventurers! Are you ready to take on the vibrant and bustling city of Toronto? Look no further, because we have compiled a list of the best activities that this city has to offer. From trendy neighborhoods to delicious food spots, get ready to experience the ultimate Toronto adventure. So grab your friends, put on your comfiest sneakers, and let's dive into the top must-see spots for all you young and adventurous souls out there. Let's go!\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(title=\"Best Activities in Toronto\", audience=\"Millenials\", tone=\"Lighthearted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP3dAgTgWq-r"
   },
   "source": [
    "## Combining Chains\n",
    "\n",
    "Often we would want to do multiple tasks using GPT. For example if we wish to generate an outline for a topic and use that outline to write a blog article we need to take the outline created from the first step and copy paste and paste as input to the second step\n",
    "\n",
    "Instead we can combine chains to achieve this in a single step. We will do this using a different type of chain called Sequential Chain. A sequential chain takes the output from one chain and passes on to the next. We will cover chains in more detail later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UqHNoQ1OXLCC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write me an outline on {topic}\",\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=-1)\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"\"\"Write a blog article in the format of the given outline \n",
    "\n",
    "    Outline:\n",
    "    {outline}\"\"\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIieZmmAXaCC",
    "outputId": "ff62bf70-503d-41b0-82a5-ea54a80a0e1d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "I. Introduction\n",
      "    A. Brief history of tennis\n",
      "    B. Popularity of tennis worldwide\n",
      "    C. Purpose of the outline\n",
      "\n",
      "II. Basics of Tennis\n",
      "    A. Court layout and dimensions\n",
      "    B. Equipment needed (racquet, balls, attire)\n",
      "    C. Scoring system\n",
      "    D. Serving rules\n",
      "    E. General rules of play\n",
      "\n",
      "III. Techniques and Fundamentals\n",
      "    A. Grip\n",
      "    B. Strokes (forehand, backhand, serve, volley)\n",
      "    C. Footwork and movement\n",
      "    D. Strategies and tactics\n",
      "    E. Practice drills to improve skills\n",
      "\n",
      "IV. Singles vs Doubles\n",
      "    A. Overview of differences\n",
      "    B. Advantages and disadvantages of each\n",
      "    C. Team dynamics in doubles\n",
      "    D. Key skills needed for success in each format\n",
      "\n",
      "V. Mental Aspect of Tennis\n",
      "    A. Importance of mental preparation\n",
      "    B. Dealing with pressure and nerves\n",
      "    C. Developing a strong mindset\n",
      "    D. Mental strategies during matches\n",
      "\n",
      "VI. Fitness and Conditioning\n",
      "    A. Physical demands of tennis\n",
      "    B. Essential fitness components for tennis players\n",
      "    C. Training exercises and drills\n",
      "    D. Importance of rest and recovery\n",
      "\n",
      "VII. Types of Tennis Tournaments\n",
      "    A. Grand Slam tournaments\n",
      "    B. ATP and WTA Tours\n",
      "    C. Other types of tournaments (challengers, futures, team events)\n",
      "    D. Qualifying and seeding process\n",
      "\n",
      "VIII. Famous Tennis Players\n",
      "    A. Men's top players (Federer, Nadal, Djokovic)\n",
      "    B. Women's top players (Serena Williams, Osaka, Barty)\n",
      "    C. Legends of the game (Sampras, Agassi, Navratilova, Graf)\n",
      "    D. Impact on the sport and their achievements\n",
      "\n",
      "IX. Tennis Etiquette\n",
      "    A. Respect for opponents and officials\n",
      "    B. Code of conduct on the court\n",
      "    C. Appropriate behavior during matches\n",
      "    D. Importance of good sportsmanship\n",
      "\n",
      "X. Current State of Tennis\n",
      "    A. Recent developments and changes in the sport\n",
      "    B. Popular tournaments and players\n",
      "    C. Technology in tennis (Hawkeye, wearable devices)\n",
      "    D. Challenges and controversies in the sport\n",
      "\n",
      "XI. Conclusion\n",
      "    A. Recap of key points\n",
      "    B. Personal reflection on the sport of tennis\n",
      "    C. Encouragement to try out or continue playing tennis. \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Introduction:\n",
      "\n",
      "Tennis is a sport that has been played for centuries, with its roots dating back to 12th century France. It began as a game played by monks, using their hands to hit a ball back and forth over a rope. However, over the years, tennis has evolved into a widely popular sport played all over the world. In this article, we will take a closer look at the basics of tennis, techniques and fundamentals, different formats, mental and physical aspects, tournaments, famous players, and etiquette. Whether you are a beginner looking to learn more about the sport or a seasoned player, this article will provide you with a comprehensive overview of all things tennis.\n",
      "\n",
      "Basics of Tennis:\n",
      "\n",
      "Before getting into the nitty-gritty, let's start with the basics of tennis. A tennis court is divided into two equal halves by a net, with lines marking the boundaries of each side. The game is played using a racquet and a ball, with the aim of hitting the ball over the net into the opponent's side of the court. The scoring system in tennis can be a bit confusing for beginners, but it follows a pattern of 15, 30, 40, and game. A player must win at least six games and have a two-game lead to win the set. The first player to win two out of three (in women's matches) or three out of five (in men's matches) sets wins the match.\n",
      "\n",
      "Techniques and Fundamentals:\n",
      "\n",
      "To play tennis effectively, one must have a good grasp of the techniques and fundamentals. The grip is the foundation of all strokes in tennis, and it is essential to have a proper grip to hit shots with control and power. The four main strokes in tennis are the forehand, backhand, serve, and volley. Footwork and movement are also crucial in tennis, as it helps with getting in the right position to hit the ball. Strategies and tactics also play a significant role in winning matches, and players must constantly think and adjust their game plan according to their opponents' strengths and weaknesses.\n",
      "\n",
      "Singles vs. Doubles:\n",
      "\n",
      "Singles and doubles are two different formats of tennis. In singles, it is a one-on-one match, while doubles involve two players on each team playing against each other. The main difference between the two is the court size, with doubles having a wider court. Doubles also require a different set of skills, as teamwork and communication are essential to success. Some players may excel in singles, while others may prefer the dynamics of doubles.\n",
      "\n",
      "Mental Aspect of Tennis:\n",
      "\n",
      "Tennis is not just a physical sport; it also requires a strong mental game. Mental preparation is crucial for success, as it helps players deal with pressure and nerves during matches. Developing a strong mindset and staying focused on the present moment is crucial in tennis, as one mistake can change the outcome of a match. It is also essential to have mental strategies in place to cope with different situations during a match.\n",
      "\n",
      "Fitness and Conditioning:\n",
      "\n",
      "Tennis is a physically demanding sport that requires a combination of strength, speed, and endurance. The essential fitness components for tennis players include agility, balance, coordination, power, and speed. Training exercises and drills specific to tennis can help players improve their overall fitness level and performance on the court. Rest and recovery are also crucial for players to avoid injuries and perform at their best.\n",
      "\n",
      "Types of Tennis Tournaments:\n",
      "\n",
      "There are various types of tennis tournaments played at different levels, from amateur to professional. The most prestigious tournaments are the four Grand Slam tournaments: The Australian Open, French Open, Wimbledon, and US Open. These are followed by the ATP (Association of Tennis Professionals) and WTA (Women's Tennis Association) tours, where top-ranked players compete for ranking points and prize money. Other types of tournaments include the Challenger and Futures events, where players can earn points to move up the rankings. Team events, such as the Davis Cup and Fed Cup, are also popular in tennis.\n",
      "\n",
      "Famous Tennis Players:\n",
      "\n",
      "Over the years, there have been many great players who have left their mark on the sport of tennis. On the men's side, names like Roger Federer, Rafael Nadal, and Novak Djokovic are synonymous with excellence and have achieved numerous records and titles. On the women's side, Serena Williams, Naomi Osaka, and Ashleigh Barty have dominated the sport in recent years. Legends of the game like Pete Sampras, Andre Agassi, Martina Navratilova, and Steffi Graf have also made their mark and continue to inspire future generations of players.\n",
      "\n",
      "Tennis Etiquette:\n",
      "\n",
      "Tennis is a sport with a strong emphasis on sportsmanship and respect for opponents and officials. The code of conduct on the court is essential, and players must adhere to it during matches. Unsportsmanlike behavior, such as verbal abuse, racquet throwing, and cheating, can result in penalties and even disqualification. It is crucial to maintain composure and show respect to opponents, regardless of the outcome of the match.\n",
      "\n",
      "Current State of Tennis:\n",
      "\n",
      "Tennis is a constantly evolving sport, with new developments and changes happening every year. Technology has also played a significant role in tennis, with the introduction of Hawkeye technology to challenge calls and wearable devices to track player performance. However, like any other sport, tennis also faces challenges and controversies, such as performance-enhancing drug use and player disputes.\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "In conclusion, tennis is a beautiful and challenging sport that requires a combination of physical and mental skills. The sport has a rich history and continues to evolve, with new players and tournaments emerging every year. Whether you are a beginner or a seasoned player, the sport of tennis offers something for everyone. So, why not grab a racquet, hit the court, and experience the thrill and beauty of tennis for yourself? \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Introduction:\n",
      "\n",
      "Tennis is a sport that has been played for centuries, with its roots dating back to 12th century France. It began as a game played by monks, using their hands to hit a ball back and forth over a rope. However, over the years, tennis has evolved into a widely popular sport played all over the world. In this article, we will take a closer look at the basics of tennis, techniques and fundamentals, different formats, mental and physical aspects, tournaments, famous players, and etiquette. Whether you are a beginner looking to learn more about the sport or a seasoned player, this article will provide you with a comprehensive overview of all things tennis.\n",
      "\n",
      "Basics of Tennis:\n",
      "\n",
      "Before getting into the nitty-gritty, let's start with the basics of tennis. A tennis court is divided into two equal halves by a net, with lines marking the boundaries of each side. The game is played using a racquet and a ball, with the aim of hitting the ball over the net into the opponent's side of the court. The scoring system in tennis can be a bit confusing for beginners, but it follows a pattern of 15, 30, 40, and game. A player must win at least six games and have a two-game lead to win the set. The first player to win two out of three (in women's matches) or three out of five (in men's matches) sets wins the match.\n",
      "\n",
      "Techniques and Fundamentals:\n",
      "\n",
      "To play tennis effectively, one must have a good grasp of the techniques and fundamentals. The grip is the foundation of all strokes in tennis, and it is essential to have a proper grip to hit shots with control and power. The four main strokes in tennis are the forehand, backhand, serve, and volley. Footwork and movement are also crucial in tennis, as it helps with getting in the right position to hit the ball. Strategies and tactics also play a significant role in winning matches, and players must constantly think and adjust their game plan according to their opponents' strengths and weaknesses.\n",
      "\n",
      "Singles vs. Doubles:\n",
      "\n",
      "Singles and doubles are two different formats of tennis. In singles, it is a one-on-one match, while doubles involve two players on each team playing against each other. The main difference between the two is the court size, with doubles having a wider court. Doubles also require a different set of skills, as teamwork and communication are essential to success. Some players may excel in singles, while others may prefer the dynamics of doubles.\n",
      "\n",
      "Mental Aspect of Tennis:\n",
      "\n",
      "Tennis is not just a physical sport; it also requires a strong mental game. Mental preparation is crucial for success, as it helps players deal with pressure and nerves during matches. Developing a strong mindset and staying focused on the present moment is crucial in tennis, as one mistake can change the outcome of a match. It is also essential to have mental strategies in place to cope with different situations during a match.\n",
      "\n",
      "Fitness and Conditioning:\n",
      "\n",
      "Tennis is a physically demanding sport that requires a combination of strength, speed, and endurance. The essential fitness components for tennis players include agility, balance, coordination, power, and speed. Training exercises and drills specific to tennis can help players improve their overall fitness level and performance on the court. Rest and recovery are also crucial for players to avoid injuries and perform at their best.\n",
      "\n",
      "Types of Tennis Tournaments:\n",
      "\n",
      "There are various types of tennis tournaments played at different levels, from amateur to professional. The most prestigious tournaments are the four Grand Slam tournaments: The Australian Open, French Open, Wimbledon, and US Open. These are followed by the ATP (Association of Tennis Professionals) and WTA (Women's Tennis Association) tours, where top-ranked players compete for ranking points and prize money. Other types of tournaments include the Challenger and Futures events, where players can earn points to move up the rankings. Team events, such as the Davis Cup and Fed Cup, are also popular in tennis.\n",
      "\n",
      "Famous Tennis Players:\n",
      "\n",
      "Over the years, there have been many great players who have left their mark on the sport of tennis. On the men's side, names like Roger Federer, Rafael Nadal, and Novak Djokovic are synonymous with excellence and have achieved numerous records and titles. On the women's side, Serena Williams, Naomi Osaka, and Ashleigh Barty have dominated the sport in recent years. Legends of the game like Pete Sampras, Andre Agassi, Martina Navratilova, and Steffi Graf have also made their mark and continue to inspire future generations of players.\n",
      "\n",
      "Tennis Etiquette:\n",
      "\n",
      "Tennis is a sport with a strong emphasis on sportsmanship and respect for opponents and officials. The code of conduct on the court is essential, and players must adhere to it during matches. Unsportsmanlike behavior, such as verbal abuse, racquet throwing, and cheating, can result in penalties and even disqualification. It is crucial to maintain composure and show respect to opponents, regardless of the outcome of the match.\n",
      "\n",
      "Current State of Tennis:\n",
      "\n",
      "Tennis is a constantly evolving sport, with new developments and changes happening every year. Technology has also played a significant role in tennis, with the introduction of Hawkeye technology to challenge calls and wearable devices to track player performance. However, like any other sport, tennis also faces challenges and controversies, such as performance-enhancing drug use and player disputes.\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "In conclusion, tennis is a beautiful and challenging sport that requires a combination of physical and mental skills. The sport has a rich history and continues to evolve, with new players and tournaments emerging every year. Whether you are a beginner or a seasoned player, the sport of tennis offers something for everyone. So, why not grab a racquet, hit the court, and experience the thrill and beauty of tennis for yourself? \n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "catchphrase = overall_chain.run(\"Tennis\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit the Prompt in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template classes in Langchain are built to make constructing prompts with dynamic inputs easier. Of these classes, the simplest is the PromptTemplate. Weâ€™ll test this by adding a single dynamic input to our previous prompt, the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can use the **format** method on our **prompt_template** to see the effect of passing a query to the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: Which libraries and model providers offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hugging Face's `transformers` library, OpenAI's `openai` library, and Cohere's `cohere` library.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(query=\"Which libraries and model providers offer LLMs?\")\n",
    "chain = prompt_template | llm\n",
    "chain.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The success of LLMs comes from their large size and ability to store â€œknowledgeâ€ within the model parameter, which is learned during model training. However, there are more ways to pass knowledge to an LLM. The two primary methods are:\n",
    "\n",
    "- Parametric knowledge â€” the knowledge mentioned above is anything that has been learned by the model during training time and is stored within the model weights (or parameters).\n",
    "- Source knowledge â€” any knowledge provided to the model at inference time via the input prompt.\n",
    "Langchainâ€™s FewShotPromptTemplate caters to source knowledge input. The idea is to â€œtrainâ€ the model on a few examples â€” we call this few-shot learning â€” and these examples are given to the model within the prompt.\n",
    "\n",
    "Few-shot learning is perfect when our model needs help understanding what weâ€™re asking it to do. We can see this in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create our examples\n",
    "examples = [ #you can have n such example of query-answer pairs\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then pass in the examples and user query, we will get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering this, we need to balance the number of examples included and our prompt size. Our hard limit is the maximum context size, but we must also consider the cost of processing more tokens through the LLM. Fewer tokens mean a cheaper service and faster completions from the LLM.\n",
    "\n",
    "The `FewShotPromptTemplate` allows us to vary the number of examples included based on these variables. First, we create a more extensive list of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, rather than passing the examples directly, we actually use a `LengthBasedExampleSelector` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50  # this sets the max length that examples should be\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass our `example_selector` to the `FewShotPromptTemplate` to create a new â€” and dynamic â€” prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we pass a shorter or longer query, we should see that the number of included examples will vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: How do birds fly?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a longer question will result in fewer examples being included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
    "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
    "what is the best way to do that?\"\"\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, weâ€™re returning fewer examples within the prompt variable. Allowing us to limit excessive token usage and avoid errors from surpassing the maximum context window of the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's revisit chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
    "\n",
    "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
    "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple utility chain. The `LLMMathChain` gives llms the ability to do math. Let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 226 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math = LLMMathChain.from_llm(llm=llm)\n",
    "\n",
    "\n",
    "count_tokens(llm_math, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enter prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question we send as input to the chain is not the only input that the llm recieves ðŸ˜‰. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! ðŸ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 32 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n13 raised to the power of 0.3432 is approximately 2.480788.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we set the prompt to only have the question we ask\n",
    "prompt = PromptTemplate(input_variables=['question'], template='{question}')\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# we ask the llm for the answer with no context\n",
    "\n",
    "count_tokens(llm_chain, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong answer! Herein lies the power of prompting and one of our most important insights so far: \n",
    "\n",
    "**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, str],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
      "        _run_manager.on_text(inputs[self.input_key])\n",
      "        llm_output = self.llm_chain.predict(\n",
      "            question=inputs[self.input_key],\n",
      "            stop=[\"```output\"],\n",
      "            callbacks=_run_manager.get_child(),\n",
      "        )\n",
      "        return self._process_llm_result(llm_output, _run_manager)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(llm_math._call))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator. We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly ðŸ”¥\n",
    "\n",
    "The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check it out](https://python.langchain.com/v0.2/docs/how_to/) and tinker with the example notebooks that you might find interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    \n",
    "    # replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    return {\"output_text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A random text with some irregular spacing.\\n Another one here as well.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now things will get interesting.\n",
    "\n",
    "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will build the prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next, initialize our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
    "\n",
    "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Chains allow us to combine multiple \n",
    "\n",
    "\n",
    "components together to create a single, coherent application. \n",
    "\n",
    "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
    "\n",
    "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
    "\n",
    "\n",
    "combining chains with other components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set. Time to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 153 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yo, check it: chains be the way we bring it all together, creating one dope app. Take user input, add some PromptTemplate flavor, then pass it to an LLM. Building chains on chains, or mixin' in other components, makes for some serious complexity. Word.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain's Chains Operations\n",
    "\n",
    "In LangChain, chains are a core concept used to combine different operations in a sequential or conditional manner. These chains allow you to build complex workflows by linking various components together. Here are some of the main types of chain operations and how they can be used:\n",
    "\n",
    "## 1. SequentialChain\n",
    "\n",
    "`SequentialChain` is used to link multiple components together so that the output of one component becomes the input for the next. This is useful for creating multi-step processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La casa es maravillosa.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI  \n",
    "\n",
    "# Define individual components\n",
    "prompt1 = PromptTemplate(template=\"Translate English to French: {text}\", input_variables=[\"text\"])\n",
    "prompt2 = PromptTemplate(template=\"Translate French to Spanish: {french_text}\", input_variables=[\"french_text\"])\n",
    "\n",
    "# Use the updated ChatOpenAI class\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
    "\n",
    "\n",
    "# Create individual LLMChains\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"french_text\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"spanish_text\")\n",
    "\n",
    "# Create a SequentialChain\n",
    "chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"spanish_text\"]  # Final output variable\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = chain({\"text\": \"The house is wonderful.\"})\n",
    "print(result[\"spanish_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLMChain\n",
    "This is one of the most commonly used chains. It's a chain that combines a language model (LLM) with a prompt template.  It is used when you want to execute a single step, such as summarization, translation, or Q&A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain: open-source framework for developing LLM-based applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(template=\"Summarize the following text in 10 words or fewer: {text}\", input_variables=[\"text\"])\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
    "\n",
    "# Create the LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "#result = chain.run({\"text\": \"LangChain is a powerful library for building language model chains.\"})\n",
    "text = \"\"\"\n",
    "LangChain is an open-source framework designed to facilitate the development of applications using large language models (LLMs). \n",
    "It provides tools to integrate various components such as prompt templates, memory, chains, and agents. \n",
    "LangChain enables developers to build chatbots, question-answering systems, and other AI-powered applications efficiently.\n",
    "\"\"\"\n",
    "\n",
    "result = chain.invoke({\"text\": text})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLMRouterChain\n",
    "\n",
    "`LLMRouterChain` helps route queries to the most appropriate large language model (LLM) or tool based on the input. It is particularly useful when working with multiple models or APIs with different capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "'''json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "'''\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "\n",
      "animals: prompt for animal expert\n",
      "vegetables: prompt for a vegetable expert\n",
      "\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include '''json at the start of the response) >>\n",
      "<< OUTPUT (must end with ''') >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router.multi_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "destinations = \"\"\"\n",
    "animals: prompt for animal expert\n",
    "vegetables: prompt for a vegetable expert\n",
    "\"\"\"\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)\n",
    "\n",
    "print(router_template.replace(\"`\", \"'\"))  # for rendering purposes\n",
    "\n",
    "\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    # Note: here we use the prompt template from above. Generally this would need\n",
    "    # to be customized.\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegetables\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"What color are carrots?\"})\n",
    "\n",
    "print(result[\"destination\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RunnableParallel\n",
    "`RunnableParallel` RunnableParallel allows running multiple chains, functions, or callables in parallel and returns a dictionary of results. It is part of the Runnable framework, which provides more flexibility and better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'french': {'question': 'Hello, how are you?', 'text': 'Bonjour, comment Ã§a va?'}, 'spanish': {'question': 'Hello, how are you?', 'text': 'Hola, Â¿cÃ³mo estÃ¡s?'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\")  \n",
    "\n",
    "# Create prompts\n",
    "prompt_1 = PromptTemplate(input_variables=[\"question\"], template=\"Translate this to French: {question}\")\n",
    "prompt_2 = PromptTemplate(input_variables=[\"question\"], template=\"Translate this to Spanish: {question}\")\n",
    "\n",
    "# Define two LLMChains\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "# Run both chains in parallel\n",
    "parallel_chain = RunnableParallel(french=chain_1, spanish=chain_2)\n",
    "\n",
    "# Example input\n",
    "result = parallel_chain.invoke({\"question\": \"Hello, how are you?\"})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MapReduceDocumentsChain\n",
    "`MapReduceChain` is a chain in LangChain used for processing large numbers of documents efficiently. It applies a map-reduce approach, where:\n",
    "\n",
    "1- Map Step: Each document is processed independently, generating intermediate responses.\n",
    "\n",
    "2- Reduce Step: These intermediate responses are aggregated to produce the final output.\n",
    "\n",
    "This chain is useful for summarization, QA over large documents, and other NLP tasks requiring multi-document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
    "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
    "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
    "]\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "# Map\n",
    "map_template = \"Write a concise summary of the following: {docs}.\"\n",
    "map_prompt = ChatPromptTemplate([(\"human\", map_template)])\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"\n",
    "The following is a set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary\n",
    "of the main themes.\n",
    "\"\"\"\n",
    "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=1000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruits exhibit distinctive colors: apples are red, blueberries are blue, and bananas are yellow.\n"
     ]
    }
   ],
   "source": [
    "result = map_reduce_chain.invoke(documents)\n",
    "\n",
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the primary chain operations available in LangChain. They can be combined and customized to create complex workflows tailored to specific language processing tasks. Each chain type serves a different purpose and can be selected based on the requirements of the task at hand."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
