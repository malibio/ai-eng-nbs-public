{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxoO0kS80FXb"
      },
      "source": [
        "# LangChain Expression Language (LCEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNqaCQXe0FXd"
      },
      "source": [
        "The **L**ang**C**hain **E**xpression **L**anguage (LCEL) is a abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer for building chains of LangChain components.\n",
        "\n",
        "LCEL comes with strong support for:\n",
        "\n",
        "1. Superfast development of chains.\n",
        "2. Advanced features such as streaming, async, parallel execution, and more.\n",
        "3. Easy integration with LangSmith and LangServe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW0fo1nCL04l"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain_openai langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Or_LL60FXf"
      },
      "source": [
        "## LCEL Syntax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev6XPSmq0FXf"
      },
      "source": [
        "To understand LCEL syntax let's first build a simple chain in typical Python syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1YYtj3TY3rb"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqVckZrR0FXf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me a small report about {topic}\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4\",\n",
        "    temperature=0.7,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "output_parser = StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3SIOgdf0FXg"
      },
      "source": [
        "In typical LangChain we would chain these together using an `LLMChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksmvw6_h0FXg",
        "outputId": "6df8db5d-8b0d-4be0-f6bd-be533945f858",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial Intelligence (AI) is a rapidly growing field of technology with the potential to make significant impacts in various sectors. It refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The concept is based on the idea of building machines capable of thinking, learning, and making decisions.\n",
            "\n",
            "AI can be categorized into two types: Narrow AI, which is designed and trained for a particular task such as voice recognition, and General AI, which can understand, learn, and apply knowledge across a broad array of tasks. \n",
            "\n",
            "AI technologies are already being used in a variety of industries including healthcare, automotive, finance, and entertainment. In healthcare, AI can help in diagnosis, drug discovery, and patient monitoring. In the automotive industry, AI is the backbone of self-driving technology. In finance, AI algorithms are used to detect fraudulent transactions and automate trading activities. \n",
            "\n",
            "Despite its many advantages, AI also has its drawbacks. There are concerns about job losses due to automation, and ethical issues related to privacy and data security. It also raises questions about decision-making, responsibility, and control when AI systems become more autonomous.\n",
            "\n",
            "However, the potential benefits of AI are immense. As technology continues to advance and evolve, AI is expected to become even more integrated into our daily lives, improving efficiency and productivity, and potentially unveiling breakthroughs in fields like healthcare, energy, and transportation. \n",
            "\n",
            "Constant research and development are being done to improve AI technology and address its limitations. With the right balance of regulation and encouragement for innovation, AI has the potential to drive significant progress in the 21st century.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=model,\n",
        "    output_parser=output_parser\n",
        ")\n",
        "\n",
        "# and run\n",
        "out = chain.run(topic=\"Artificial Intelligence\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeiVLTAd0FXg"
      },
      "source": [
        "Using LCEL the format is different, rather than relying on `Chains` we simple chain together each component using the pipe operator `|`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsYoR0yd0FXg",
        "outputId": "6f65585f-eff6-4646-e41c-aaabea593a70",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: A Brief Report on Artificial Intelligence \n",
            "\n",
            "Artificial Intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks normally requiring human intelligence. These tasks include learning and adapting to new information, understanding human language, recognizing patterns, solving problems, and making decisions.\n",
            "\n",
            "AI has been categorized into two types: Narrow AI, designed to perform a narrow task (e.g., facial recognition or internet searches) and General AI, which can perform any intellectual task that a human being can do.\n",
            "\n",
            "Machine Learning (ML), a subset of AI, involves the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world. Deep Learning, a subset of ML, uses neural networks with several layers (known as deep neural networks) to carry out the process of machine learning.\n",
            "\n",
            "AI has been integrated into various sectors including healthcare, finance, transportation, and more. It has been pivotal in the development of autonomous vehicles, predictive analytics, customer service robots, and virtual personal assistants such as Siri and Alexa.\n",
            "\n",
            "Despite its advantages, AI also poses significant challenges. There are concerns over job displacement due to automation, privacy issues due to the growing use of personal data, and ethical considerations in programming AI systems.\n",
            "\n",
            "In the future, AI is expected to continue evolving, leading to advancements in areas such as personalized education, climate change modeling, and even space exploration. However, it is crucial that as AI continues to develop, considerations are made to mitigate potential risks and ensure its use benefits all of humanity.\n"
          ]
        }
      ],
      "source": [
        "lcel_chain = prompt | model | output_parser\n",
        "\n",
        "# and run\n",
        "out = lcel_chain.invoke({\"topic\": \"Artificial Intelligence\"})\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNCyT8qh0FXg"
      },
      "source": [
        "Pretty cool, the way that this pipe operator is being used is that it is taking output from the function to the _left_ and feeding it into the function on the _right_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfX_5s0E0FXg"
      },
      "source": [
        "## How the Pipe Operator Works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "080xdKe00FXg"
      },
      "source": [
        "To really understand LCEL we can take a look at how this pipe operation works. We know it takes output from the _right_ and feeds it to the _left_ — but this isn't typical Python, so how is this implemented? Let's try creating our own version of this with some simple functions.\n",
        "\n",
        "We will be using the `__or__` method within Python class objects. When we place two classes together like `chain = class_a | class_b` the Python interpreter will check if these classes contain this `__or__` method. If they do then our `|` code will be translated into `chain = class_a.__or__(class_b)`.\n",
        "\n",
        "That means both of these patterns will return the same thing:\n",
        "\n",
        "```python\n",
        "# object approach\n",
        "chain = class_a.__or__(class_b)\n",
        "chain(\"some input\")\n",
        "\n",
        "# pipe approach\n",
        "chain = class_a | class_b\n",
        "chain(\"some input\")\n",
        "\n",
        "```\n",
        "\n",
        "With that in mind, we can build a `Runnable` class that consumes a function and turns it into a function that can be chained with other functions using the pipe operator `|`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "u6hkdHo_0FXh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Runnable:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "\n",
        "    def __or__(self, other):\n",
        "        def chained_func(*args, **kwargs):\n",
        "            # the other func consumes the result of this func\n",
        "            return other(self.func(*args, **kwargs))\n",
        "        return Runnable(chained_func)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlRIk_5y0FXh"
      },
      "source": [
        "Let's implement this to take the value `3`, add `5` (giving `8`), and multiply by `2` (giving `16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSqD0cQb0FXh",
        "outputId": "f7b6c217-75e1-4925-cf9f-cdda807a5662",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def add_five(x):\n",
        "    return x + 5\n",
        "\n",
        "def multiply_by_two(x):\n",
        "    return x * 2\n",
        "\n",
        "# wrap the functions with Runnable\n",
        "add_five = Runnable(add_five)\n",
        "multiply_by_two = Runnable(multiply_by_two)\n",
        "\n",
        "# run them using the object approach\n",
        "chain = add_five.__or__(multiply_by_two)\n",
        "chain(3)  # should return 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fXO74X30FXh"
      },
      "source": [
        "Using `__or__` directly we get the correct answer, now let's try using the pipe operator `|` to chain them together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6yCIDR-0FXh",
        "outputId": "850ef189-a878-470d-9c4d-b1a647902f74",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chain the runnable functions together\n",
        "chain = add_five | multiply_by_two\n",
        "\n",
        "# invoke the chain\n",
        "chain(3)  # we should return 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq3gUEtl0FXh"
      },
      "source": [
        "Using either method we get the same response, at it's core this is what LCEL is doing, but there is more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3rHW1O40FXi"
      },
      "source": [
        "## LCEL Deep Dive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjyqioeM0FXi"
      },
      "source": [
        "Now that we understand what this syntax is doing under the hood, let's explore it within the context of LCEL and see a few of the additional methods that LangChain has provided to maximize flexibility when working with LCEL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5uXrUBV0FXi"
      },
      "source": [
        "### Runnables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qASmq_oF0FXi"
      },
      "source": [
        "When working with LCEL we may find that we need to modify the structure or values being passed between components — for this we can use _runnables_. Let's try:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw6JaLAYXdYA"
      },
      "outputs": [],
      "source": [
        "!pip install docarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7ei2kj2rXKie"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "embedding = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
        "    [\"half the info will be here\", \"James' birthday is the 7th December\"],\n",
        "    embedding=embedding\n",
        ")\n",
        "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
        "    [\"and half here\", \"James was born in 1994\"],\n",
        "    embedding=embedding\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtOo_UXr0FXi"
      },
      "source": [
        "First let's try passing a question through to a single one of these `vecstore` objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IHZRT9L-0FXi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough\n",
        ")\n",
        "\n",
        "retriever_a = vecstore_a.as_retriever()\n",
        "retriever_b = vecstore_b.as_retriever()\n",
        "\n",
        "prompt_str = \"\"\"Answer the question below using the context:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\"context\": retriever_a, \"question\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "chain = retrieval | prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnSIIHNJ0FXi"
      },
      "source": [
        "We use two new objects here, `RunnableParallel` and `RunnablePassthrough`. The `RunnableParallel` object allows us to define multiple values and operations, and run them all in parallel. Here we call `retriever_a` using the input to our chain (below), and then pass the results from `retriever_a` to the next component in the `chain` via the `\"context\"` parameter.\n",
        "\n",
        "<div>\n",
        "<img src=\"./img/lecl.webp\" alt='auto' width=\"1000\"/>\n",
        "</div>\n",
        "\n",
        "The `RunnablePassthrough` object is used as a `\"passthrough\"` take takes any input to the current component (`retrieval`) and allows us to provide it in the component output via the `\"question\"` key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG4UYWJC0FXi",
        "outputId": "5596b9ab-14bc-4cf5-df8a-f9b077e746a9",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "James was born on the 7th of December.\n"
          ]
        }
      ],
      "source": [
        "out = chain.invoke(\"when was James born?\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHui3Q_r0FXi"
      },
      "source": [
        "Here we have used `RunnableParallel` to create two parallel streams of information, one for `\"context\"` that is information fed in from `retriever_a`, and another for `\"question\"` which is the _passthrough_ information, ie the information that is passed through from our `chain.invoke(\"when was James born?\")` call.\n",
        "\n",
        "Using this information the chain is close to answering the question but it doesn't have enough information, it is missing the information that we have stored in `retriever_b`. Fortunately, we can have multiple parallel information streams using the `RunnableParallel` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vadD7CVK0FXj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt_str = \"\"\"Answer the question below using the context:\n",
        "\n",
        "Context:\n",
        "{context_a}\n",
        "{context_b}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"context_a\": retriever_a, \"context_b\": retriever_b,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")\n",
        "\n",
        "chain = retrieval | prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVfE1NHQ0FXj",
        "outputId": "bf24b4f3-8f6b-4b4b-a455-ce3678f25fca",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "James was born on the 7th December 1994.\n"
          ]
        }
      ],
      "source": [
        "out = chain.invoke(\"when was James born?\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6oLrT920FXj"
      },
      "source": [
        "Now the `RunnableParallel` object is allowing us to search with `retriever_a` _and_ `retriever_b` in parallel, ie at the same time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OZe4owl0FXj"
      },
      "source": [
        "## Runnable Lambdas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-SVacgX0FXj"
      },
      "source": [
        "The `RunnableLambda` is a LangChain abstraction that allows us to turn Python functions into pipe-compatible function, similar to the `Runnable` class we created near the beginning of this notebook.\n",
        "\n",
        "Let's try it out with our earlier `add_five` and `multiply_by_two` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bExgeOoz0FXj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# wrap the functions with RunnableLambda\n",
        "add_five = RunnableLambda(add_five)\n",
        "multiply_by_two = RunnableLambda(multiply_by_two)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNYrWuYe0FXj"
      },
      "source": [
        "As with our earlier `Runnable` abstraction, we can use `|` operators to chain together our `RunnableLambda` abstractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7YrmPU8S0FXk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "chain = add_five | multiply_by_two"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EAiFcjb0FXk"
      },
      "source": [
        "Unlike our `Runnable` abstraction, we cannot run the `RunnableLambda` chain by calling it directly, instead we must call `chain.invoke`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q88p8GKJ0FXk",
        "outputId": "c783bc54-1ef9-4dd1-ccd9-101d49318bef",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JulSPOif0FXk"
      },
      "source": [
        "As before, we can see the same answer. Naturally we can feed custom functions into our chains using this approach. Let's try a short chain and see where we might want to insert a custom function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hlQlh2Rk0FXk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt_str = \"Tell me an short fact about {topic}\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "chain = prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1aPqHRoI0FXk",
        "outputId": "77adb15c-baaa-44b3-a97e-dc2aaa297224",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial intelligence refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and linguistic intelligence.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WFuDI_Os0FXk",
        "outputId": "4bfe7c9b-5098-4a3f-98bf-a2dff55d8318",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial Intelligence (AI) was first coined as a term in 1956 at the Dartmouth Conference, the first conference dedicated to the subject.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pAX9xTb0FXl"
      },
      "source": [
        "The returned text always includes the initial `\"Here's a short fact about ...\\n\\n\"` — let's add a function to split on double newlines `\"\\n\\n\"` and only return the fact itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_NZD1Wda0FXl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def extract_fact(x):\n",
        "    if \"\\n\\n\" in x:\n",
        "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "get_fact = RunnableLambda(extract_fact)\n",
        "\n",
        "chain = prompt | model | output_parser | get_fact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ib2djJJ50FXl",
        "outputId": "dc3a0b31-9373-4ab8-c455-b65522bb3f62",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial Intelligence was first coined as a term in 1956 by John McCarthy at the Dartmouth Conference.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "pTUQ6Bn30FXl",
        "outputId": "c5d999be-76ef-4105-ce83-62f9c27c2b41",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial Intelligence (AI) simulates human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1xTUR-J0FXl"
      },
      "source": [
        "Now we're getting well formatted outputs using our final custom `get_fact` function.\n",
        "\n",
        "---\n",
        "\n",
        "That covers the essentials you need to getting started and building with the **L**ang**C**hain **E**xpression **L**anguage (LCEL). With it, we can put together chains very easily — and the current focus of the LangChain team is on further LCEL development and support.\n",
        "\n",
        "The pros and cons of LCEL are varied. Those that love it tend to focus on the minimalist code style, LCEL's support for streaming, parallel operations, and async, and LCEL's nice integration with LangChain's focus on chaining components together.\n",
        "\n",
        "There are also people that are less fond of LCEL. Typically people will point to it being yet another abstraction on top of an already very abstract library, that the syntax is confusing, against [the Zen of Python](https://peps.python.org/pep-0020/), and requires too much effort to learn new (or uncommon) syntax.\n",
        "\n",
        "Both viewpoints are entirely valid, LCEL is a very different approach — ofcourse there are major pros and major cons. In either case, if you're willing to spend some time learning the syntax, it allows us to develop very quickly, and with that in mind it's well worth learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElA19yS30FXl"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
